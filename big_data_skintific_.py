# -*- coding: utf-8 -*-
"""Big Data Skintific..ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18sUDGczSHVD9r0v3RwO_U1sNVj1yM_W3

Impor Dataset Hasil Scrapping
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

file_path = '/content/drive/MyDrive/Dataset_Big_Data/Clean dataset fix.xlsx'
df = pd.read_excel(file_path)
df.head()

"""Preprocessing"""

# Hapus duplikat dan nilai kosong
df = df.drop_duplicates()
df = df.dropna(subset=['Final Text'])
df.reset_index(drop=True, inplace=True)

# Cek hasil teks setelah preprocessing
df['Final Text'].head()

# Tambahkan panjang karakter dan jumlah kata
df['char_length'] = df['Final Text'].apply(len)
df['word_count'] = df['Final Text'].apply(lambda x: len(x.split()))

# Statistik
print(df[['char_length', 'word_count']].describe())

# Kata paling sering muncul
from collections import Counter
word_freq = Counter(' '.join(df['Final Text']).split())
print(word_freq.most_common(10))

"""Visualisasi Distribusi Panjang Teks & Visualisasi WordCloud"""

import matplotlib.pyplot as plt
import seaborn as sns

# Distribusi panjang teks
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.histplot(df['char_length'], bins=30, kde=True)
plt.title("Distribusi Panjang Karakter")

plt.subplot(1, 2, 2)
sns.histplot(df['word_count'], bins=30, kde=True)
plt.title("Distribusi Jumlah Kata")

plt.tight_layout()
plt.show()

# WordCloud
from wordcloud import WordCloud

text = ' '.join(df['Final Text'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("WordCloud")
plt.show()

"""Korelasi Word Count dan Character Length"""

import seaborn as sns
import matplotlib.pyplot as plt

# Korelasi
corr = df[['word_count', 'char_length']].corr()

# Heatmap
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Korelasi antara Word Count dan Character Length")
plt.show()

"""Instalasi dan Impor Pustaka untuk Analisis Teks"""

!pip install textblob deep-translator openpyxl --quiet

import pandas as pd
from textblob import TextBlob
from deep_translator import GoogleTranslator
import time
import sys

df = pd.read_excel(file_path)

if 'Final Text' not in df.columns:
    raise ValueError("Kolom 'Final Text' tidak ditemukan dalam dataset!")

df = df.dropna(subset=['Final Text']).reset_index(drop=True)

"""Terjemahan Teks Menggunakan Google Translate"""

def translate_text(text):
    try:
        if not text or pd.isna(text):
            return ''
        if len(text) > 5000:
            text = text[:5000]
        return GoogleTranslator(source='auto', target='en').translate(str(text))
    except Exception as e:
        print("Gagal translate:", e)
        time.sleep(1)
        return ''

print("Mulai menerjemahkan...")
total_rows = len(df)
translated_text = []

for idx, text in enumerate(df['Final Text'], start=1):
    sys.stdout.write(f"\rMenerjemahkan baris ke-{idx}/{total_rows}")
    sys.stdout.flush()
    translated_text.append(translate_text(text))

df['Final Text (en)'] = translated_text
print(f"\nTerjemahan selesai. Telah menerjemahkan {len(translated_text)} baris dari {total_rows}.")

"""Analisis Sentimen dengan TextBlob"""

# Simpan hasil ke file Excel
df.to_excel('hasil_terjemahan_sentimen.xlsx', index=False)
print("Hasil disimpan ke 'hasil_terjemahan_sentimen.xlsx'")

# Tampilkan 10 baris pertama untuk pengecekan
from IPython.display import display

# Ambil kolom yang diperlukan dan tampilkan sebagian
selected_columns = df[['Final Text', 'Final Text (en)', 'Sentiment']]
display(selected_columns.head(10))

# Fungsi analisis sentimen berdasarkan teks terjemahan
def get_sentiment(text):
    try:
        polarity = TextBlob(str(text)).sentiment.polarity
        if polarity > 0:
            return 'positive'
        elif polarity < 0:
            return 'negative'
        else:
            return 'neutral'
    except:
        return 'neutral'

df['Sentiment'] = df['Final Text (en)'].apply(get_sentiment)
sentiment_counts = df['Sentiment'].value_counts()

print("Jumlah sentimen:")
print(sentiment_counts)

plt.figure(figsize=(6, 6))
sentiment_counts.plot.pie(autopct='%1.1f%%', colors=['lightgreen', 'lightblue', 'salmon'])
plt.title('Persentase Sentimen')
plt.ylabel('')
plt.show()

plt.figure(figsize=(6, 4))
sns.countplot(x='Sentiment', data=df, palette='Set2', order=['positive', 'neutral', 'negative'])
plt.title('Distribusi Sentimen')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Pisahkan teks berdasarkan sentimen
positive_text = " ".join(df[df['Sentiment'] == 'positive']['Final Text (en)'].dropna().astype(str))
negative_text = " ".join(df[df['Sentiment'] == 'negative']['Final Text (en)'].dropna().astype(str))
neutral_text  = " ".join(df[df['Sentiment'] == 'neutral']['Final Text (en)'].dropna().astype(str))

# Buat WordCloud untuk setiap sentimen
wordcloud_pos = WordCloud(width=400, height=400, background_color='white', colormap='summer').generate(positive_text)
wordcloud_neg = WordCloud(width=400, height=400, background_color='white', colormap='autumn').generate(negative_text)
wordcloud_neu = WordCloud(width=400, height=400, background_color='white', colormap='cool').generate(neutral_text)

# Tampilkan dalam 1 baris 3 kolom
fig, axs = plt.subplots(1, 3, figsize=(18, 6))

axs[0].imshow(wordcloud_pos, interpolation='bilinear')
axs[0].set_title('Positive')
axs[0].axis('off')

axs[1].imshow(wordcloud_neg, interpolation='bilinear')
axs[1].set_title('Negative')
axs[1].axis('off')

axs[2].imshow(wordcloud_neu, interpolation='bilinear')
axs[2].set_title('Neutral')
axs[2].axis('off')

plt.suptitle("Fig 5. Wordcloud Sentiment", fontsize=16)
plt.tight_layout()
plt.show()

"""Ekstraksi Fitur dengan TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Lakukan TF-IDF pada kolom hasil terjemahan
tfidf = TfidfVectorizer(
    max_features=1000,
    min_df=3,
    max_df=0.7,
    stop_words='english',
    ngram_range=(1, 3)
)

X = tfidf.fit_transform(df['Final Text (en)'])
y = df['Sentiment']

"""Pelatihan dan Evaluasi Model Naive Bayes"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import time

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=0
)

# Buat model Naive Bayes
model_nb = MultinomialNB()

# Training
t1 = time.time()
model_nb.fit(X_train, y_train)
t2 = time.time()

# Prediksi
y_pred = model_nb.predict(X_test)
t3 = time.time()

# Waktu
train_time = t2 - t1
predict_time = t3 - t2
print(f"Naive Bayes Training time: {train_time:.4f}s; Prediction time: {predict_time:.4f}s")

# Evaluasi
print("\nClassification Report:")
print(classification_report(y_test, y_pred, zero_division=0))
print("Akurasi:", accuracy_score(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=model_nb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_nb.classes_)

# Visualisasi
disp.plot(cmap='Blues')
plt.title("Confusion Matrix - Naive Bayes")
plt.grid(False)
plt.show()